{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re, random\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "from itertools import combinations \n",
    "from nltk.corpus import stopwords as stopwords_nltk \n",
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import re\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdsd_labeled_filepaths = glob('/media/dmlab/My Passport/DATA/BenchmarkDataset/MDSD/*_labeled_*.csv')\n",
    "mdsd_unlabeled_filepaths = glob('/media/dmlab/My Passport/DATA/BenchmarkDataset/MDSD/*_unlabeled_*.csv')\n",
    "save_dir = '/media/dmlab/My Passport/DATA/cross-domain/data'\n",
    "if not os.path.exists(save_dir): os.makedirs(save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 4,000 unlabeled reviews per domain\n",
    "* 2,000 labeled reviews per domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[books] Droped 477734 rows having duplicated text\n",
      "[dvd] Droped 39540 rows having duplicated text\n",
      "[electronics] Droped 2213 rows having duplicated text\n",
      "[kitchen] Droped 1218 rows having duplicated text\n",
      "Created /media/dmlab/My Passport/DATA/cross-domain/data/MDSD_unlabeled.json\n",
      "Created /media/dmlab/My Passport/DATA/cross-domain/data/MDSD_labeled.json\n"
     ]
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "MULTIPLE_SPACES = re.compile(' +', re.UNICODE)\n",
    "removal_list = \"|,‘, ’, ◇, ‘, ”,  ’, ·, \\“, ·, △, ➤, ●,  , ■, (, ), \\\", >>, `, /, -,∼,=,ㆍ<,>, ?, !,【,】, …, ◆,%\"\n",
    "stopwords = stopwords_nltk.words('english')\n",
    "\n",
    "def concat_dataframes(filepaths, drop_duplicates=True, sample_num=4000):\n",
    "    dfs = []\n",
    "    for filepath in filepaths:\n",
    "        df = pd.read_csv(filepath)\n",
    "        domain = os.path.basename(filepath).split('_')[0]\n",
    "        df['domain'] = domain\n",
    "        original_len = len(df)\n",
    "        if drop_duplicates:\n",
    "            df.drop_duplicates(['text'], keep='last', inplace=True)   # 중복된 텍스트 제거\n",
    "            print('[{}] Droped {} rows having duplicated text'.format(domain, original_len-len(df)))\n",
    "        if sample_num is not None:\n",
    "            df = df.sample(n=sample_num)   # 4000개 데이터만 랜덤하게 선택\n",
    "        dfs.append(df)\n",
    "    concat_df = pd.concat(dfs)\n",
    "    concat_df = shuffle(concat_df)   # Shuffle\n",
    "    concat_df.reset_index(inplace=True)   # Reset index\n",
    "    return concat_df\n",
    "\n",
    "unlabeled_df = concat_dataframes(mdsd_unlabeled_filepaths)\n",
    "filepath = os.path.join(save_dir, 'MDSD_unlabeled.json')\n",
    "unlabeled_df.to_json(filepath)\n",
    "print('Created {}'.format(filepath))\n",
    "\n",
    "labeled_df = concat_dataframes(mdsd_labeled_filepaths, drop_duplicates=False, sample_num=None)\n",
    "filepath = os.path.join(save_dir, 'MDSD_labeled.json')\n",
    "labeled_df.to_json(filepath)\n",
    "print('Created {}'.format(filepath))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>text</th>\n",
       "      <th>domain</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12429</td>\n",
       "      <td>the ease and timelyness of the product was ver...</td>\n",
       "      <td>electronics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>813196</td>\n",
       "      <td>I'm a man about to turn 50 with 4 yr. olds twi...</td>\n",
       "      <td>books</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>527488</td>\n",
       "      <td>There is tons to do in Washington DC and despi...</td>\n",
       "      <td>books</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>701823</td>\n",
       "      <td>Good grounding book in strategy</td>\n",
       "      <td>books</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>14177</td>\n",
       "      <td>super excellent cd player with remote control/...</td>\n",
       "      <td>electronics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15995</th>\n",
       "      <td>597945</td>\n",
       "      <td>I work in the healthcare field and have seen q...</td>\n",
       "      <td>books</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15996</th>\n",
       "      <td>9786</td>\n",
       "      <td>I have a miniature poodle puppy who eats every...</td>\n",
       "      <td>kitchen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15997</th>\n",
       "      <td>606221</td>\n",
       "      <td>I wasnt a big fan of this book. it didnt keep ...</td>\n",
       "      <td>books</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15998</th>\n",
       "      <td>30590</td>\n",
       "      <td>This movie has one the best performances by Ja...</td>\n",
       "      <td>dvd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15999</th>\n",
       "      <td>9038</td>\n",
       "      <td>Quick delivery. Reasonable price. Item could n...</td>\n",
       "      <td>kitchen</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        index                                               text       domain\n",
       "0       12429  the ease and timelyness of the product was ver...  electronics\n",
       "1      813196  I'm a man about to turn 50 with 4 yr. olds twi...        books\n",
       "2      527488  There is tons to do in Washington DC and despi...        books\n",
       "3      701823                    Good grounding book in strategy        books\n",
       "4       14177  super excellent cd player with remote control/...  electronics\n",
       "...       ...                                                ...          ...\n",
       "15995  597945  I work in the healthcare field and have seen q...        books\n",
       "15996    9786  I have a miniature poodle puppy who eats every...      kitchen\n",
       "15997  606221  I wasnt a big fan of this book. it didnt keep ...        books\n",
       "15998   30590  This movie has one the best performances by Ja...          dvd\n",
       "15999    9038  Quick delivery. Reasonable price. Item could n...      kitchen\n",
       "\n",
       "[16000 rows x 3 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unlabeled_df = pd.read_json(os.path.join(save_dir, 'MDSD_unlabeled.json'))\n",
    "unlabeled_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>domain</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>366</td>\n",
       "      <td>Elaine Pagels is a wonderful writer. Her expla...</td>\n",
       "      <td>positive</td>\n",
       "      <td>books</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1024</td>\n",
       "      <td>I baked six loaves in this machine. I have bee...</td>\n",
       "      <td>negative</td>\n",
       "      <td>kitchen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>548</td>\n",
       "      <td>I BOUGHT THIS LIKE A IPOD BUT ITS BETTER I HAV...</td>\n",
       "      <td>positive</td>\n",
       "      <td>electronics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>675</td>\n",
       "      <td>These are called \"Fruit Bowl\" but they are the...</td>\n",
       "      <td>positive</td>\n",
       "      <td>kitchen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1098</td>\n",
       "      <td>Corkscrew has a nice heavy duty feel and does ...</td>\n",
       "      <td>negative</td>\n",
       "      <td>kitchen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7995</th>\n",
       "      <td>156</td>\n",
       "      <td>This soap dish is beautiful, practical and stu...</td>\n",
       "      <td>positive</td>\n",
       "      <td>kitchen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7996</th>\n",
       "      <td>462</td>\n",
       "      <td>WE ARE HALFWAY THROUGH THE FRIENDS SERIES...5 ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>dvd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7997</th>\n",
       "      <td>314</td>\n",
       "      <td>I was a little hesitant to spend over $10 for ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>kitchen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7998</th>\n",
       "      <td>635</td>\n",
       "      <td>Excellent item, very powerful and stylish. Wou...</td>\n",
       "      <td>positive</td>\n",
       "      <td>kitchen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7999</th>\n",
       "      <td>519</td>\n",
       "      <td>this is a good cartoon for competition with ro...</td>\n",
       "      <td>positive</td>\n",
       "      <td>dvd</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8000 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      index                                               text     label  \\\n",
       "0       366  Elaine Pagels is a wonderful writer. Her expla...  positive   \n",
       "1      1024  I baked six loaves in this machine. I have bee...  negative   \n",
       "2       548  I BOUGHT THIS LIKE A IPOD BUT ITS BETTER I HAV...  positive   \n",
       "3       675  These are called \"Fruit Bowl\" but they are the...  positive   \n",
       "4      1098  Corkscrew has a nice heavy duty feel and does ...  negative   \n",
       "...     ...                                                ...       ...   \n",
       "7995    156  This soap dish is beautiful, practical and stu...  positive   \n",
       "7996    462  WE ARE HALFWAY THROUGH THE FRIENDS SERIES...5 ...  positive   \n",
       "7997    314  I was a little hesitant to spend over $10 for ...  positive   \n",
       "7998    635  Excellent item, very powerful and stylish. Wou...  positive   \n",
       "7999    519  this is a good cartoon for competition with ro...  positive   \n",
       "\n",
       "           domain  \n",
       "0           books  \n",
       "1         kitchen  \n",
       "2     electronics  \n",
       "3         kitchen  \n",
       "4         kitchen  \n",
       "...           ...  \n",
       "7995      kitchen  \n",
       "7996          dvd  \n",
       "7997      kitchen  \n",
       "7998      kitchen  \n",
       "7999          dvd  \n",
       "\n",
       "[8000 rows x 4 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labeled_df = pd.read_json(os.path.join(save_dir, 'MDSD_labeled.json'))\n",
    "labeled_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Logistic Regression으로 Source/Target 분류하는 문제 풀게 한 후, Coefficient가 높은 단어를 domain-specific feature로 취급\n",
    "    - (Source-Target) 쌍 구분 유의\n",
    "    - https://github.com/tapilab/emnlp-2020-spurious/blob/5ae0b718f7bbf6216731453e4e59a72f38d90bb5/Step1_get_matched_sentences.ipynb 참고"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    def __init__(self, X, y, vec, df, moniker):\n",
    "        \"\"\"\n",
    "        X: feature matrix;\n",
    "        y: labels;\n",
    "        vec: CountVectorizer\n",
    "        df: dataframe\n",
    "        feats: features from CountVectorizer\n",
    "        moniker: reference name to the dataset\n",
    "        \"\"\"\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.vec = vec\n",
    "        self.df = df\n",
    "        self.feats = np.array(vec.get_feature_names())\n",
    "        self.moniker = moniker # name of the dataset\n",
    "        \n",
    "def simple_vectorize(df):\n",
    "    \"\"\"\n",
    "    Vectorize text\n",
    "    \"\"\"\n",
    "    vec = CountVectorizer(min_df=5, binary=True, max_df=.8)\n",
    "    X = vec.fit_transform(df.text)\n",
    "    y = df.label.values\n",
    "    feats = np.array(vec.get_feature_names())\n",
    "    \n",
    "    return X, y, vec, feats\n",
    "\n",
    "def print_coef(clf, feats, n=100):\n",
    "    \"\"\"\n",
    "    sort and print words by coef stregth (abs(coef))\n",
    "    \"\"\"\n",
    "    if len(clf.classes_) == 2:\n",
    "        coefs = [-1*clf.coef_[0], clf.coef_[0]] # change the coef relation corresponding with each class\n",
    "    else:\n",
    "        coefs = clf.coef_\n",
    "\n",
    "    records = []\n",
    "    for label, coef in zip(clf.classes_, coefs):\n",
    "        topi = coef.argsort()[::-1][:n]\n",
    "        records.extend([(label,f,c) for f, c in zip(feats[topi], coef[topi])])\n",
    "    return pd.DataFrame(records, columns=['label', 'word', 'coef'])\n",
    "        \n",
    "def get_top_terms(dataset, top_n):\n",
    "    \"\"\"\n",
    "    Fit classifier, print top-n terms;\n",
    "    Top features (features have high coef): abs(coef) >= thresh\n",
    "    Placebos (features have low coef): abs(coef) <= thresh\n",
    "    \"\"\"\n",
    "    clf = LogisticRegression(class_weight='auto', C=1, solver='lbfgs', max_iter=1000)\n",
    "    clf.fit(dataset.X, dataset.y)\n",
    "    \n",
    "    coef_df = print_coef(clf, dataset.feats, n=top_n)\n",
    "    return coef_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created /media/dmlab/My Passport/DATA/cross-domain/data/MDSD_books&electronics_keywords.csv\n",
      "Created /media/dmlab/My Passport/DATA/cross-domain/data/MDSD_dvd&electronics_keywords.csv\n",
      "Created /media/dmlab/My Passport/DATA/cross-domain/data/MDSD_electronics&kitchen_keywords.csv\n",
      "Created /media/dmlab/My Passport/DATA/cross-domain/data/MDSD_books&dvd_keywords.csv\n",
      "Created /media/dmlab/My Passport/DATA/cross-domain/data/MDSD_books&kitchen_keywords.csv\n",
      "Created /media/dmlab/My Passport/DATA/cross-domain/data/MDSD_dvd&kitchen_keywords.csv\n"
     ]
    }
   ],
   "source": [
    "for (domain1, domain2) in list(combinations(unlabeled_df.domain.unique(), 2)):\n",
    "    moniker = '&'.join(sorted([domain1, domain2]))\n",
    "    df = copy.copy(unlabeled_df[unlabeled_df['domain'].isin([domain1, domain2])])\n",
    "    df = df[['text', 'domain']]\n",
    "    df.columns = ['text', 'label']\n",
    "    X, y, vec, feats = simple_vectorize(df)\n",
    "    ds = Dataset(X, y, vec, df, moniker) # construct dataset object\n",
    "\n",
    "    keyword_df = get_top_terms(ds, top_n=100)\n",
    "    filepath = os.path.join(save_dir, 'MDSD_{}_keywords.csv'.format(moniker))\n",
    "    keyword_df.to_csv(filepath, index=False)\n",
    "    print('Created {}'.format(filepath))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Post-training용 데이터셋\n",
    "    - 도메인 2개 (파일명에 알파벳 순으로 기재)\n",
    "    - Proposed Post-training option\n",
    "        - ds-keyword: 소스/타겟의 domain-specific features 전체를 [UNK]로 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8000/8000 [00:38<00:00, 210.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created /media/dmlab/My Passport/DATA/cross-domain/data/MDSD_books&electronics_ds-keyword_for_post.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8000/8000 [00:41<00:00, 193.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created /media/dmlab/My Passport/DATA/cross-domain/data/MDSD_dvd&electronics_ds-keyword_for_post.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8000/8000 [00:29<00:00, 272.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created /media/dmlab/My Passport/DATA/cross-domain/data/MDSD_electronics&kitchen_ds-keyword_for_post.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8000/8000 [00:46<00:00, 171.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created /media/dmlab/My Passport/DATA/cross-domain/data/MDSD_books&dvd_ds-keyword_for_post.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8000/8000 [00:35<00:00, 224.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created /media/dmlab/My Passport/DATA/cross-domain/data/MDSD_books&kitchen_ds-keyword_for_post.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8000/8000 [00:38<00:00, 206.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created /media/dmlab/My Passport/DATA/cross-domain/data/MDSD_dvd&kitchen_ds-keyword_for_post.txt\n"
     ]
    }
   ],
   "source": [
    "def mask_keywords(doc, keywords):\n",
    "    words = doc.split()\n",
    "    for i in range(len(words)):\n",
    "        for keyword in keywords:\n",
    "            if keyword.lower() in words[i].lower():\n",
    "                words[i] = '[UNK]'\n",
    "    return ' '.join(words)\n",
    "\n",
    "def create_txt_for_post_training(docs, save_filepath, num_of_duplicates=10):    \n",
    "    with open(save_filepath, 'w') as output_file:\n",
    "        for _ in range(num_of_duplicates): # each sentence in the target domain gets duplicated 10 times\n",
    "            for doc_idx, doc in enumerate(docs):\n",
    "                output_file.write('{}\\n\\n'.format(doc))\n",
    "        output_file.write('[EOD]')\n",
    "    print(f'Created {save_filepath}')\n",
    "    \n",
    "mode = 'ds-keyword'\n",
    "    \n",
    "domains = unlabeled_df.domain.unique()\n",
    "for (domain1, domain2) in list(combinations(domains, 2)):\n",
    "    moniker = '&'.join(sorted([domain1, domain2]))\n",
    "    df = copy.copy(unlabeled_df[unlabeled_df['domain'].isin([domain1, domain2])])\n",
    "    keyword_df = pd.read_csv(os.path.join(save_dir, 'MDSD_{}_keywords.csv'.format(moniker)))    \n",
    "    df['masked_text'] = df['text'].progress_apply(lambda x: mask_keywords(x, keyword_df.word.values))\n",
    "    docs = df['masked_text'].values\n",
    "    \n",
    "    save_filepath = os.path.join(save_dir, 'MDSD_{}_{}_for_post.txt'.format('&'.join(sorted([domain1, domain2])), mode))\n",
    "    create_txt_for_post_training(docs, save_filepath)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchtext",
   "language": "python",
   "name": "torchtext"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
