{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re, random\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "from itertools import combinations \n",
    "from nltk.corpus import stopwords as stopwords_nltk \n",
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import re\n",
    "from keybert import KeyBERT\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdsd_labeled_filepaths = glob('/media/dmlab/My Passport/DATA/BenchmarkDataset/MDSD/*_labeled_*.csv')\n",
    "mdsd_unlabeled_filepaths = glob('/media/dmlab/My Passport/DATA/BenchmarkDataset/MDSD/*_unlabeled_*.csv')\n",
    "save_dir = '/media/dmlab/My Passport/DATA/cross-domain/data'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 4,000 unlabeled reviews per domain\n",
    "* 2,000 labeled reviews per domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[books] Droped 477734 rows having duplicated text\n",
      "[dvd] Droped 39540 rows having duplicated text\n",
      "[electronics] Droped 2213 rows having duplicated text\n",
      "[kitchen] Droped 1218 rows having duplicated text\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16000/16000 [25:23<00:00, 10.50it/s]\n",
      "100%|██████████| 16000/16000 [00:01<00:00, 10075.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created /media/dmlab/My Passport/DATA/cross-domain/MDSD_unlabeled.json\n",
      "[books] Droped 18 rows having duplicated text\n",
      "[dvd] Droped 33 rows having duplicated text\n",
      "[electronics] Droped 39 rows having duplicated text\n",
      "[kitchen] Droped 23 rows having duplicated text\n",
      "Created /media/dmlab/My Passport/DATA/cross-domain/MDSD_labeled.json\n"
     ]
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "MULTIPLE_SPACES = re.compile(' +', re.UNICODE)\n",
    "removal_list = \"|,‘, ’, ◇, ‘, ”,  ’, ·, \\“, ·, △, ➤, ●,  , ■, (, ), \\\", >>, `, /, -,∼,=,ㆍ<,>, ?, !,【,】, …, ◆,%\"\n",
    "stopwords = stopwords_nltk.words('english')\n",
    "kw_model = KeyBERT()\n",
    "\n",
    "def concat_dataframes(filepaths, sample_num=None):\n",
    "    dfs = []\n",
    "    for filepath in filepaths:\n",
    "        df = pd.read_csv(filepath)\n",
    "        domain = os.path.basename(filepath).split('_')[0]\n",
    "        df['domain'] = domain\n",
    "        original_len = len(df)\n",
    "        df.drop_duplicates(['text'], keep='last', inplace=True)   # 중복된 텍스트 제거\n",
    "        print('[{}] Droped {} rows having duplicated text'.format(domain, original_len-len(df)))\n",
    "        if sample_num is not None:\n",
    "            df = df.sample(n=sample_num)   # 4000개 데이터만 랜덤하게 선택\n",
    "        dfs.append(df)\n",
    "    concat_df = pd.concat(dfs)\n",
    "    concat_df = shuffle(concat_df)   # Shuffle\n",
    "    concat_df.reset_index(inplace=True)   # Reset index\n",
    "    return concat_df\n",
    "\n",
    "def prepare_unlabeled_data(filepaths):\n",
    "    def get_preprocessed_tokens(text):\n",
    "        text = text.translate(str.maketrans(removal_list, ' '*len(removal_list)))   # 특수문자 제거\n",
    "        text = re.sub(MULTIPLE_SPACES, ' ', text)   # 무의미한 공백 제거\n",
    "        words = word_tokenize(text.lower())   # 소문자로 변경 후 tokenization\n",
    "        nouns = [token for token, tag in pos_tag(words) if tag in ['NN', 'NNS', 'NNP', 'NNPS']]   # 명사 추출\n",
    "        nouns = [lemmatizer.lemmatize(token) for token in nouns]   # lemmatization (e.g., movies -> movie)\n",
    "        nouns = [token for token in nouns if token not in stopwords]   # 불용어 제거\n",
    "        nouns = [token for token in nouns if len(token)>1]   # 길이가 1 이하인 단어 제거\n",
    "        return nouns\n",
    "\n",
    "    def mask_keywords(doc, keywords):\n",
    "        words = doc.split()\n",
    "        for i in range(len(words)):\n",
    "            for keyword in keywords:\n",
    "                if keyword.lower() in words[i].lower():\n",
    "                    words[i] = '[UNK]'\n",
    "        return ' '.join(words)\n",
    "    \n",
    "    def random_masked_text(num_of_unks, doc):\n",
    "        words = doc.split()\n",
    "        completed_unks = 0\n",
    "        while True:\n",
    "            random_idx = random.choice(range(len(words)))\n",
    "            if completed_unks == num_of_unks: break\n",
    "            if words[random_idx] != '[UNK]':\n",
    "                words[random_idx] = '[UNK]'\n",
    "                completed_unks += 1\n",
    "        return ' '.join(words)\n",
    "\n",
    "    concat_df = concat_dataframes(filepaths, sample_num=4000)\n",
    "\n",
    "    # Keyword 추출 using KeyBERT\n",
    "    concat_df['keywords'] = concat_df['text'].progress_apply(lambda x: \\\n",
    "        [word for (word, score) in kw_model.extract_keywords(x) if word in get_preprocessed_tokens(x)])\n",
    "    \n",
    "    # Keyword masking\n",
    "    concat_df['masked_text'] = concat_df.progress_apply(lambda x: mask_keywords(x['text'], x['keywords']), axis=1)\n",
    "    \n",
    "    # Random word masking\n",
    "    # keyword 개수 만큼 랜덤하게 단어를 골라서 [UNK]로 처리\n",
    "    concat_df['random_masked_text'] = concat_df.progress_apply(lambda x: \\\n",
    "        random_masked_text(x['masked_text'].count('[UNK]'), x['text']), axis=1)\n",
    "    return concat_df\n",
    "\n",
    "unlabeled_df = prepare_unlabeled_data(mdsd_unlabeled_filepaths)\n",
    "filepath = os.path.join(save_dir, 'MDSD_unlabeled.json')\n",
    "unlabeled_df.to_json(filepath)\n",
    "print('Created {}'.format(filepath))\n",
    "\n",
    "labeled_df = concat_dataframes(mdsd_labeled_filepaths)\n",
    "filepath = os.path.join(save_dir, 'MDSD_labeled.json')\n",
    "labeled_df.to_json(filepath)\n",
    "print('Created {}'.format(filepath))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Post-training용 데이터셋\n",
    "    - 도메인 2개 (파일명에 알파벳 순으로 기재)\n",
    "    - Post-training options\n",
    "        1. Raw (baseline)\n",
    "        2. keyword\n",
    "        3. random word (baseline): keyword 개수 만큼 랜덤하게 단어를 골라서 [UNK]로 처리\n",
    "    \n",
    "> When generating the post-training data, each sentence in the target domain gets duplicated 10 times with different masks ~~and sentences pair~~."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created /media/dmlab/My Passport/DATA/cross-domain/data/MDSD_dvd&electronics_raw_for_post.txt\n",
      "Created /media/dmlab/My Passport/DATA/cross-domain/data/MDSD_electronics&kitchen_raw_for_post.txt\n",
      "Created /media/dmlab/My Passport/DATA/cross-domain/data/MDSD_books&electronics_raw_for_post.txt\n",
      "Created /media/dmlab/My Passport/DATA/cross-domain/data/MDSD_dvd&kitchen_raw_for_post.txt\n",
      "Created /media/dmlab/My Passport/DATA/cross-domain/data/MDSD_books&dvd_raw_for_post.txt\n",
      "Created /media/dmlab/My Passport/DATA/cross-domain/data/MDSD_books&kitchen_raw_for_post.txt\n",
      "Created /media/dmlab/My Passport/DATA/cross-domain/data/MDSD_dvd&electronics_keyword_for_post.txt\n",
      "Created /media/dmlab/My Passport/DATA/cross-domain/data/MDSD_electronics&kitchen_keyword_for_post.txt\n",
      "Created /media/dmlab/My Passport/DATA/cross-domain/data/MDSD_books&electronics_keyword_for_post.txt\n",
      "Created /media/dmlab/My Passport/DATA/cross-domain/data/MDSD_dvd&kitchen_keyword_for_post.txt\n",
      "Created /media/dmlab/My Passport/DATA/cross-domain/data/MDSD_books&dvd_keyword_for_post.txt\n",
      "Created /media/dmlab/My Passport/DATA/cross-domain/data/MDSD_books&kitchen_keyword_for_post.txt\n",
      "Created /media/dmlab/My Passport/DATA/cross-domain/data/MDSD_dvd&electronics_random_for_post.txt\n",
      "Created /media/dmlab/My Passport/DATA/cross-domain/data/MDSD_electronics&kitchen_random_for_post.txt\n",
      "Created /media/dmlab/My Passport/DATA/cross-domain/data/MDSD_books&electronics_random_for_post.txt\n",
      "Created /media/dmlab/My Passport/DATA/cross-domain/data/MDSD_dvd&kitchen_random_for_post.txt\n",
      "Created /media/dmlab/My Passport/DATA/cross-domain/data/MDSD_books&dvd_random_for_post.txt\n",
      "Created /media/dmlab/My Passport/DATA/cross-domain/data/MDSD_books&kitchen_random_for_post.txt\n"
     ]
    }
   ],
   "source": [
    "def create_txt_for_post_training(docs, save_filepath, num_of_duplicates=10):    \n",
    "    with open(save_filepath, 'w') as output_file:\n",
    "        for _ in range(num_of_duplicates): # each sentence in the target domain gets duplicated 10 times\n",
    "            for doc_idx, doc in enumerate(docs):\n",
    "                output_file.write('{}\\n\\n'.format(doc))\n",
    "        output_file.write('[EOD]')\n",
    "    print(f'Created {save_filepath}')\n",
    "    \n",
    "domains = unlabeled_df.domain.unique()\n",
    "for mode in ['raw', 'keyword', 'random']:\n",
    "    for (domain1, domain2) in list(combinations(domains, 2)):\n",
    "        df = unlabeled_df[unlabeled_df['domain'].isin([domain1, domain2])]\n",
    "        if mode == 'raw':\n",
    "            docs = df['text'].values\n",
    "        elif mode == 'keyword':\n",
    "            docs = df['masked_text'].values\n",
    "        elif mode == 'random':\n",
    "            docs = df['random_masked_text'].values\n",
    "\n",
    "        save_filepath = os.path.join(save_dir, 'MDSD_{}_{}_for_post.txt'.format('&'.join(sorted([domain1, domain2])), mode))\n",
    "        create_txt_for_post_training(docs, save_filepath)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchtext",
   "language": "python",
   "name": "torchtext"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
