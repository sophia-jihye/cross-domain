{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re, random\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "from itertools import combinations \n",
    "from nltk.corpus import stopwords as stopwords_nltk \n",
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import re\n",
    "from keybert import KeyBERT\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdsd_labeled_filepaths = glob('/media/dmlab/My Passport/DATA/BenchmarkDataset/MDSD/*_labeled_*.csv')\n",
    "mdsd_unlabeled_filepaths = glob('/media/dmlab/My Passport/DATA/BenchmarkDataset/MDSD/*_unlabeled_*.csv')\n",
    "save_dir = '/media/dmlab/My Passport/DATA/cross-domain/data'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 4,000 unlabeled reviews per domain\n",
    "* 2,000 labeled reviews per domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8000/8000 [12:31<00:00, 10.64it/s]\n",
      "100%|██████████| 8000/8000 [00:00<00:00, 9415.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created /media/dmlab/My Passport/DATA/cross-domain/data/MDSD_labeled.json\n"
     ]
    }
   ],
   "source": [
    "# lemmatizer = WordNetLemmatizer()\n",
    "# MULTIPLE_SPACES = re.compile(' +', re.UNICODE)\n",
    "# removal_list = \"|,‘, ’, ◇, ‘, ”,  ’, ·, \\“, ·, △, ➤, ●,  , ■, (, ), \\\", >>, `, /, -,∼,=,ㆍ<,>, ?, !,【,】, …, ◆,%\"\n",
    "# stopwords = stopwords_nltk.words('english')\n",
    "# kw_model = KeyBERT()\n",
    "\n",
    "def prepare_data(filepaths, sample_num=4000, drop_duplicates=True, process_random_masked=True):\n",
    "    def concat_dataframes(filepaths, sample_num=None):\n",
    "        dfs = []\n",
    "        for filepath in filepaths:\n",
    "            df = pd.read_csv(filepath)\n",
    "            domain = os.path.basename(filepath).split('_')[0]\n",
    "            df['domain'] = domain\n",
    "            original_len = len(df)\n",
    "            if drop_duplicates:\n",
    "                df.drop_duplicates(['text'], keep='last', inplace=True)   # 중복된 텍스트 제거\n",
    "                print('[{}] Droped {} rows having duplicated text'.format(domain, original_len-len(df)))\n",
    "            if sample_num is not None:\n",
    "                df = df.sample(n=sample_num)   # 4000개 데이터만 랜덤하게 선택\n",
    "            dfs.append(df)\n",
    "        concat_df = pd.concat(dfs)\n",
    "        concat_df = shuffle(concat_df)   # Shuffle\n",
    "        concat_df.reset_index(inplace=True)   # Reset index\n",
    "        return concat_df\n",
    "\n",
    "    def get_preprocessed_tokens(text):\n",
    "        text = text.translate(str.maketrans(removal_list, ' '*len(removal_list)))   # 특수문자 제거\n",
    "        text = re.sub(MULTIPLE_SPACES, ' ', text)   # 무의미한 공백 제거\n",
    "        words = word_tokenize(text.lower())   # 소문자로 변경 후 tokenization\n",
    "        nouns = [token for token, tag in pos_tag(words) if tag in ['NN', 'NNS', 'NNP', 'NNPS']]   # 명사 추출\n",
    "        nouns = [lemmatizer.lemmatize(token) for token in nouns]   # lemmatization (e.g., movies -> movie)\n",
    "        nouns = [token for token in nouns if token not in stopwords]   # 불용어 제거\n",
    "        nouns = [token for token in nouns if len(token)>1]   # 길이가 1 이하인 단어 제거\n",
    "        return nouns\n",
    "\n",
    "    def mask_keywords(doc, keywords):\n",
    "        words = doc.split()\n",
    "        for i in range(len(words)):\n",
    "            for keyword in keywords:\n",
    "                if keyword.lower() in words[i].lower():\n",
    "                    words[i] = '[UNK]'\n",
    "        return ' '.join(words)\n",
    "    \n",
    "    def random_masked_text(num_of_unks, doc):\n",
    "        words = doc.split()\n",
    "        completed_unks = 0\n",
    "        while True:\n",
    "            random_idx = random.choice(range(len(words)))\n",
    "            if completed_unks == num_of_unks: break\n",
    "            if words[random_idx] != '[UNK]':\n",
    "                words[random_idx] = '[UNK]'\n",
    "                completed_unks += 1\n",
    "        return ' '.join(words)\n",
    "\n",
    "    concat_df = concat_dataframes(filepaths, sample_num=sample_num)\n",
    "\n",
    "    # Keyword 추출 using KeyBERT\n",
    "    concat_df['keywords'] = concat_df['text'].progress_apply(lambda x: \\\n",
    "        [word for (word, score) in kw_model.extract_keywords(x) if word in get_preprocessed_tokens(x)])\n",
    "    \n",
    "    # Keyword masking\n",
    "    concat_df['masked_text'] = concat_df.progress_apply(lambda x: mask_keywords(x['text'], x['keywords']), axis=1)\n",
    "    \n",
    "    # Random word masking\n",
    "    # keyword 개수 만큼 랜덤하게 단어를 골라서 [UNK]로 처리\n",
    "    if process_random_masked:\n",
    "        concat_df['random_masked_text'] = concat_df.progress_apply(lambda x: \\\n",
    "            random_masked_text(x['masked_text'].count('[UNK]'), x['text']), axis=1)\n",
    "    return concat_df\n",
    "\n",
    "# unlabeled_df = prepare_data(mdsd_unlabeled_filepaths)\n",
    "# filepath = os.path.join(save_dir, 'MDSD_unlabeled.json')\n",
    "# unlabeled_df.to_json(filepath)\n",
    "# print('Created {}'.format(filepath))\n",
    "\n",
    "labeled_df = prepare_data(mdsd_labeled_filepaths, sample_num=None, \\\n",
    "                          drop_duplicates=False, process_random_masked=False)\n",
    "filepath = os.path.join(save_dir, 'MDSD_labeled.json')\n",
    "labeled_df.to_json(filepath)\n",
    "print('Created {}'.format(filepath))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>text</th>\n",
       "      <th>domain</th>\n",
       "      <th>keywords</th>\n",
       "      <th>masked_text</th>\n",
       "      <th>random_masked_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3152</td>\n",
       "      <td>Have had this TV for about a week and am very ...</td>\n",
       "      <td>electronics</td>\n",
       "      <td>[lcd, glare, tv]</td>\n",
       "      <td>Have had this [UNK] for about a week and am ve...</td>\n",
       "      <td>Have had this TV for about a week and am very ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11330</td>\n",
       "      <td>I have been a big fan of these printers. I own...</td>\n",
       "      <td>electronics</td>\n",
       "      <td>[labelwriters, labelwriter]</td>\n",
       "      <td>I have been a big fan of these printers. I own...</td>\n",
       "      <td>I have been a big fan of these printers. I own...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>55284</td>\n",
       "      <td>I was first introduced to the Salsa Crazy seri...</td>\n",
       "      <td>dvd</td>\n",
       "      <td>[salsa, dance, routine, costume]</td>\n",
       "      <td>I was first introduced to the [UNK] Crazy seri...</td>\n",
       "      <td>I was first introduced to the Salsa Crazy seri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>39147</td>\n",
       "      <td>Right off the bat, i've been a fan since 1979,...</td>\n",
       "      <td>dvd</td>\n",
       "      <td>[dvd, ct, trick, band]</td>\n",
       "      <td>Right off the bat, i've been a fan since 1979,...</td>\n",
       "      <td>[UNK] [UNK] the bat, i've been a fan [UNK] [UN...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>112780</td>\n",
       "      <td>A welcome Return for Michael Myers; one of the...</td>\n",
       "      <td>dvd</td>\n",
       "      <td>[divimax, myers, dvd]</td>\n",
       "      <td>A welcome Return for Michael [UNK] one of the ...</td>\n",
       "      <td>A welcome Return for Michael Myers; one of the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15995</th>\n",
       "      <td>8293</td>\n",
       "      <td>The Memorex 10OZ 152A AIR DUSTER is a lifesave...</td>\n",
       "      <td>electronics</td>\n",
       "      <td>[keyboard, duster, dust]</td>\n",
       "      <td>The Memorex 10OZ 152A AIR [UNK] is a lifesaver...</td>\n",
       "      <td>The Memorex 10OZ 152A AIR DUSTER is a lifesave...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15996</th>\n",
       "      <td>15340</td>\n",
       "      <td>This would of been a great sd card had it actu...</td>\n",
       "      <td>electronics</td>\n",
       "      <td>[mda, card]</td>\n",
       "      <td>This would of been a great sd [UNK] had it act...</td>\n",
       "      <td>This would of been a great sd card had it actu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15997</th>\n",
       "      <td>90158</td>\n",
       "      <td>This box totally rox my sox! The extra 100 bux...</td>\n",
       "      <td>dvd</td>\n",
       "      <td>[box, sox, bux, disx, collecter]</td>\n",
       "      <td>This [UNK] totally rox my [UNK] The extra 100 ...</td>\n",
       "      <td>[UNK] box [UNK] rox my sox! [UNK] extra 100 bu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15998</th>\n",
       "      <td>110836</td>\n",
       "      <td>Who knew that Gordon from Sesame Street was a ...</td>\n",
       "      <td>dvd</td>\n",
       "      <td>[pimp, gordon, movie, street]</td>\n",
       "      <td>Who knew that [UNK] from Sesame [UNK] was a [U...</td>\n",
       "      <td>Who knew that Gordon [UNK] [UNK] Street was a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15999</th>\n",
       "      <td>684232</td>\n",
       "      <td>The Twentieth-Century World: An International ...</td>\n",
       "      <td>books</td>\n",
       "      <td>[geopolitics, century, history]</td>\n",
       "      <td>The [UNK] World: An International [UNK] by Dr....</td>\n",
       "      <td>The Twentieth-Century World: An International ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16000 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        index                                               text       domain  \\\n",
       "0        3152  Have had this TV for about a week and am very ...  electronics   \n",
       "1       11330  I have been a big fan of these printers. I own...  electronics   \n",
       "2       55284  I was first introduced to the Salsa Crazy seri...          dvd   \n",
       "3       39147  Right off the bat, i've been a fan since 1979,...          dvd   \n",
       "4      112780  A welcome Return for Michael Myers; one of the...          dvd   \n",
       "...       ...                                                ...          ...   \n",
       "15995    8293  The Memorex 10OZ 152A AIR DUSTER is a lifesave...  electronics   \n",
       "15996   15340  This would of been a great sd card had it actu...  electronics   \n",
       "15997   90158  This box totally rox my sox! The extra 100 bux...          dvd   \n",
       "15998  110836  Who knew that Gordon from Sesame Street was a ...          dvd   \n",
       "15999  684232  The Twentieth-Century World: An International ...        books   \n",
       "\n",
       "                               keywords  \\\n",
       "0                      [lcd, glare, tv]   \n",
       "1           [labelwriters, labelwriter]   \n",
       "2      [salsa, dance, routine, costume]   \n",
       "3                [dvd, ct, trick, band]   \n",
       "4                 [divimax, myers, dvd]   \n",
       "...                                 ...   \n",
       "15995          [keyboard, duster, dust]   \n",
       "15996                       [mda, card]   \n",
       "15997  [box, sox, bux, disx, collecter]   \n",
       "15998     [pimp, gordon, movie, street]   \n",
       "15999   [geopolitics, century, history]   \n",
       "\n",
       "                                             masked_text  \\\n",
       "0      Have had this [UNK] for about a week and am ve...   \n",
       "1      I have been a big fan of these printers. I own...   \n",
       "2      I was first introduced to the [UNK] Crazy seri...   \n",
       "3      Right off the bat, i've been a fan since 1979,...   \n",
       "4      A welcome Return for Michael [UNK] one of the ...   \n",
       "...                                                  ...   \n",
       "15995  The Memorex 10OZ 152A AIR [UNK] is a lifesaver...   \n",
       "15996  This would of been a great sd [UNK] had it act...   \n",
       "15997  This [UNK] totally rox my [UNK] The extra 100 ...   \n",
       "15998  Who knew that [UNK] from Sesame [UNK] was a [U...   \n",
       "15999  The [UNK] World: An International [UNK] by Dr....   \n",
       "\n",
       "                                      random_masked_text  \n",
       "0      Have had this TV for about a week and am very ...  \n",
       "1      I have been a big fan of these printers. I own...  \n",
       "2      I was first introduced to the Salsa Crazy seri...  \n",
       "3      [UNK] [UNK] the bat, i've been a fan [UNK] [UN...  \n",
       "4      A welcome Return for Michael Myers; one of the...  \n",
       "...                                                  ...  \n",
       "15995  The Memorex 10OZ 152A AIR DUSTER is a lifesave...  \n",
       "15996  This would of been a great sd card had it actu...  \n",
       "15997  [UNK] box [UNK] rox my sox! [UNK] extra 100 bu...  \n",
       "15998  Who knew that Gordon [UNK] [UNK] Street was a ...  \n",
       "15999  The Twentieth-Century World: An International ...  \n",
       "\n",
       "[16000 rows x 6 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unlabeled_df = pd.read_json(os.path.join(save_dir, 'MDSD_unlabeled.json'))\n",
    "unlabeled_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>domain</th>\n",
       "      <th>keywords</th>\n",
       "      <th>masked_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>494</td>\n",
       "      <td>I mean that statement in two ways. First, in m...</td>\n",
       "      <td>positive</td>\n",
       "      <td>dvd</td>\n",
       "      <td>[goodfellas, hd, dvd]</td>\n",
       "      <td>I mean that statement in two ways. First, in m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>455</td>\n",
       "      <td>I bought the cooler for my Dell Inspiron which...</td>\n",
       "      <td>positive</td>\n",
       "      <td>electronics</td>\n",
       "      <td>[dell, cooler, inspiron, fan, pavilion]</td>\n",
       "      <td>I bought the [UNK] for my [UNK] [UNK] which te...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>745</td>\n",
       "      <td>I'm so happy with these speakers. For the pric...</td>\n",
       "      <td>positive</td>\n",
       "      <td>electronics</td>\n",
       "      <td>[bass, sound]</td>\n",
       "      <td>I'm so happy with these speakers. For the pric...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>740</td>\n",
       "      <td>I found the film, \"Malice\" to be a very intrig...</td>\n",
       "      <td>positive</td>\n",
       "      <td>dvd</td>\n",
       "      <td>[murder, malice, killer]</td>\n",
       "      <td>I found the film, [UNK] to be a very intriguin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>259</td>\n",
       "      <td>This pan is a good example of Calphalon qualit...</td>\n",
       "      <td>positive</td>\n",
       "      <td>kitchen</td>\n",
       "      <td>[pan, calphalon, oven, stove, roast]</td>\n",
       "      <td>This [UNK] is a good example of [UNK] quality ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7995</th>\n",
       "      <td>1128</td>\n",
       "      <td>It takes indeed some real power to get so many...</td>\n",
       "      <td>negative</td>\n",
       "      <td>books</td>\n",
       "      <td>[book, paradigm, innovation]</td>\n",
       "      <td>It takes indeed some real power to get so many...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7996</th>\n",
       "      <td>268</td>\n",
       "      <td>The Penguin Guide is still the best in the bus...</td>\n",
       "      <td>positive</td>\n",
       "      <td>books</td>\n",
       "      <td>[penguin, guide, music, comprehensiveness]</td>\n",
       "      <td>The [UNK] [UNK] is still the best in the busin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7997</th>\n",
       "      <td>783</td>\n",
       "      <td>I got this pan because I've been coveting All-...</td>\n",
       "      <td>positive</td>\n",
       "      <td>kitchen</td>\n",
       "      <td>[pan, cookware, stove, clad]</td>\n",
       "      <td>I got this [UNK] because I've been coveting [U...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7998</th>\n",
       "      <td>1048</td>\n",
       "      <td>The scale does not come with a battery, even t...</td>\n",
       "      <td>negative</td>\n",
       "      <td>kitchen</td>\n",
       "      <td>[battery, scale, cell, load]</td>\n",
       "      <td>The [UNK] does not come with a [UNK] even thou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7999</th>\n",
       "      <td>626</td>\n",
       "      <td>I purchase this on Amazon for $499 no tax &amp; fr...</td>\n",
       "      <td>positive</td>\n",
       "      <td>electronics</td>\n",
       "      <td>[lcd, hdmi, samsung, hd, price]</td>\n",
       "      <td>I purchase this on Amazon for $499 no tax &amp; fr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8000 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      index                                               text     label  \\\n",
       "0       494  I mean that statement in two ways. First, in m...  positive   \n",
       "1       455  I bought the cooler for my Dell Inspiron which...  positive   \n",
       "2       745  I'm so happy with these speakers. For the pric...  positive   \n",
       "3       740  I found the film, \"Malice\" to be a very intrig...  positive   \n",
       "4       259  This pan is a good example of Calphalon qualit...  positive   \n",
       "...     ...                                                ...       ...   \n",
       "7995   1128  It takes indeed some real power to get so many...  negative   \n",
       "7996    268  The Penguin Guide is still the best in the bus...  positive   \n",
       "7997    783  I got this pan because I've been coveting All-...  positive   \n",
       "7998   1048  The scale does not come with a battery, even t...  negative   \n",
       "7999    626  I purchase this on Amazon for $499 no tax & fr...  positive   \n",
       "\n",
       "           domain                                    keywords  \\\n",
       "0             dvd                       [goodfellas, hd, dvd]   \n",
       "1     electronics     [dell, cooler, inspiron, fan, pavilion]   \n",
       "2     electronics                               [bass, sound]   \n",
       "3             dvd                    [murder, malice, killer]   \n",
       "4         kitchen        [pan, calphalon, oven, stove, roast]   \n",
       "...           ...                                         ...   \n",
       "7995        books                [book, paradigm, innovation]   \n",
       "7996        books  [penguin, guide, music, comprehensiveness]   \n",
       "7997      kitchen                [pan, cookware, stove, clad]   \n",
       "7998      kitchen                [battery, scale, cell, load]   \n",
       "7999  electronics             [lcd, hdmi, samsung, hd, price]   \n",
       "\n",
       "                                            masked_text  \n",
       "0     I mean that statement in two ways. First, in m...  \n",
       "1     I bought the [UNK] for my [UNK] [UNK] which te...  \n",
       "2     I'm so happy with these speakers. For the pric...  \n",
       "3     I found the film, [UNK] to be a very intriguin...  \n",
       "4     This [UNK] is a good example of [UNK] quality ...  \n",
       "...                                                 ...  \n",
       "7995  It takes indeed some real power to get so many...  \n",
       "7996  The [UNK] [UNK] is still the best in the busin...  \n",
       "7997  I got this [UNK] because I've been coveting [U...  \n",
       "7998  The [UNK] does not come with a [UNK] even thou...  \n",
       "7999  I purchase this on Amazon for $499 no tax & fr...  \n",
       "\n",
       "[8000 rows x 6 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labeled_df = pd.read_json(os.path.join(save_dir, 'MDSD_labeled.json'))\n",
    "labeled_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Post-training용 데이터셋\n",
    "    - 도메인 2개 (파일명에 알파벳 순으로 기재)\n",
    "    - Post-training options\n",
    "        1. Raw (baseline)\n",
    "        2. keyword\n",
    "        3. random word (baseline): keyword 개수 만큼 랜덤하게 단어를 골라서 [UNK]로 처리\n",
    "    \n",
    "> When generating the post-training data, each sentence in the target domain gets duplicated 10 times with different masks ~~and sentences pair~~."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created /media/dmlab/My Passport/DATA/cross-domain/data/MDSD_dvd&electronics_raw_for_post.txt\n",
      "Created /media/dmlab/My Passport/DATA/cross-domain/data/MDSD_electronics&kitchen_raw_for_post.txt\n",
      "Created /media/dmlab/My Passport/DATA/cross-domain/data/MDSD_books&electronics_raw_for_post.txt\n",
      "Created /media/dmlab/My Passport/DATA/cross-domain/data/MDSD_dvd&kitchen_raw_for_post.txt\n",
      "Created /media/dmlab/My Passport/DATA/cross-domain/data/MDSD_books&dvd_raw_for_post.txt\n",
      "Created /media/dmlab/My Passport/DATA/cross-domain/data/MDSD_books&kitchen_raw_for_post.txt\n",
      "Created /media/dmlab/My Passport/DATA/cross-domain/data/MDSD_dvd&electronics_keyword_for_post.txt\n",
      "Created /media/dmlab/My Passport/DATA/cross-domain/data/MDSD_electronics&kitchen_keyword_for_post.txt\n",
      "Created /media/dmlab/My Passport/DATA/cross-domain/data/MDSD_books&electronics_keyword_for_post.txt\n",
      "Created /media/dmlab/My Passport/DATA/cross-domain/data/MDSD_dvd&kitchen_keyword_for_post.txt\n",
      "Created /media/dmlab/My Passport/DATA/cross-domain/data/MDSD_books&dvd_keyword_for_post.txt\n",
      "Created /media/dmlab/My Passport/DATA/cross-domain/data/MDSD_books&kitchen_keyword_for_post.txt\n",
      "Created /media/dmlab/My Passport/DATA/cross-domain/data/MDSD_dvd&electronics_random_for_post.txt\n",
      "Created /media/dmlab/My Passport/DATA/cross-domain/data/MDSD_electronics&kitchen_random_for_post.txt\n",
      "Created /media/dmlab/My Passport/DATA/cross-domain/data/MDSD_books&electronics_random_for_post.txt\n",
      "Created /media/dmlab/My Passport/DATA/cross-domain/data/MDSD_dvd&kitchen_random_for_post.txt\n",
      "Created /media/dmlab/My Passport/DATA/cross-domain/data/MDSD_books&dvd_random_for_post.txt\n",
      "Created /media/dmlab/My Passport/DATA/cross-domain/data/MDSD_books&kitchen_random_for_post.txt\n"
     ]
    }
   ],
   "source": [
    "def create_txt_for_post_training(docs, save_filepath, num_of_duplicates=10):    \n",
    "    with open(save_filepath, 'w') as output_file:\n",
    "        for _ in range(num_of_duplicates): # each sentence in the target domain gets duplicated 10 times\n",
    "            for doc_idx, doc in enumerate(docs):\n",
    "                output_file.write('{}\\n\\n'.format(doc))\n",
    "        output_file.write('[EOD]')\n",
    "    print(f'Created {save_filepath}')\n",
    "    \n",
    "domains = unlabeled_df.domain.unique()\n",
    "for mode in ['raw', 'keyword', 'random']:\n",
    "    for (domain1, domain2) in list(combinations(domains, 2)):\n",
    "        df = unlabeled_df[unlabeled_df['domain'].isin([domain1, domain2])]\n",
    "        if mode == 'raw':\n",
    "            docs = df['text'].values\n",
    "        elif mode == 'keyword':\n",
    "            docs = df['masked_text'].values\n",
    "        elif mode == 'random':\n",
    "            docs = df['random_masked_text'].values\n",
    "\n",
    "        save_filepath = os.path.join(save_dir, 'MDSD_{}_{}_for_post.txt'.format('&'.join(sorted([domain1, domain2])), mode))\n",
    "        create_txt_for_post_training(docs, save_filepath)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchtext",
   "language": "python",
   "name": "torchtext"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
